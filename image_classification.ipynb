{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyFSmxu7KRr2O/9ViMyUWW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.Load the CIFAR-10 dataset: You can load the dataset using the Keras library, which provides a convenient function to load the dataset and split it into training and testing sets"
      ],
      "metadata": {
        "id": "gMuq22L_dijf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the CIFAR10 dataset\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "(train_images, train_labels),(test_images, test_labels)= cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAEh6KGmcG3n",
        "outputId": "6ae35481-ee8e-493f-ff37-fc4d4aa7b3ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Preprocess the data: You need to preprocess the data by normalizing the pixel values to the range of 0 to 1 and converting the labels to one-hot encoding"
      ],
      "metadata": {
        "id": "dJbN01sFdHqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import float32\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "#Normalize pixel values\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "#Convert the labels to one-hot encoding\n",
        "num_classes = 10\n",
        "train_labels = to_categorical(train_labels, num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes)"
      ],
      "metadata": {
        "id": "qmGghw2SddUD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Define the model architecture: You can define a simple CNN architecture using the Keras Sequential API"
      ],
      "metadata": {
        "id": "X7ZXUBTtiaZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation = 'relu', input_shape =(32, 32, 3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation = 'relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64,(3,3), activation = 'relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation = 'relu'),\n",
        "    Dense(num_classes, activation= 'softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "ShGs7xExioGx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above model has three convolutional layers followed by max pooling layers, a flattening layer, two fully connected layers, and an output layer with 10 units and softmax activation."
      ],
      "metadata": {
        "id": "OJwPkHtIo7B9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Compile the model: You need to compile the model by specifying the loss function, optimizer, and evaluation metrics. "
      ],
      "metadata": {
        "id": "H4zFLSaqpMTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer= Adam(learning_rate=0.005),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fFNq4YpYo9Nb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.Train the model: You can train the model on the training data using the fit method"
      ],
      "metadata": {
        "id": "cxtVfpnZqCrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels, epochs=50, batch_size= 64,\n",
        "          validation_data= (test_images, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRL7njOeqE5o",
        "outputId": "41668f57-0100-430b-bc4e-86a64e4c3f19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "782/782 [==============================] - 78s 100ms/step - loss: 1.4040 - accuracy: 0.4899 - val_loss: 1.3843 - val_accuracy: 0.4955\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 1.3073 - accuracy: 0.5296 - val_loss: 1.3289 - val_accuracy: 0.5317\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 75s 96ms/step - loss: 1.2326 - accuracy: 0.5599 - val_loss: 1.2695 - val_accuracy: 0.5468\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 1.1780 - accuracy: 0.5794 - val_loss: 1.2273 - val_accuracy: 0.5666\n",
            "Epoch 5/50\n",
            "782/782 [==============================] - 77s 98ms/step - loss: 1.1506 - accuracy: 0.5909 - val_loss: 1.2848 - val_accuracy: 0.5568\n",
            "Epoch 6/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 1.1182 - accuracy: 0.6027 - val_loss: 1.1551 - val_accuracy: 0.5937\n",
            "Epoch 7/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 1.0892 - accuracy: 0.6142 - val_loss: 1.2312 - val_accuracy: 0.5738\n",
            "Epoch 8/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 1.0773 - accuracy: 0.6182 - val_loss: 1.1708 - val_accuracy: 0.5879\n",
            "Epoch 9/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 1.0645 - accuracy: 0.6226 - val_loss: 1.1484 - val_accuracy: 0.5962\n",
            "Epoch 10/50\n",
            "782/782 [==============================] - 81s 103ms/step - loss: 1.0477 - accuracy: 0.6277 - val_loss: 1.1503 - val_accuracy: 0.5966\n",
            "Epoch 11/50\n",
            "782/782 [==============================] - 75s 96ms/step - loss: 1.0397 - accuracy: 0.6312 - val_loss: 1.1264 - val_accuracy: 0.6030\n",
            "Epoch 12/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 1.0281 - accuracy: 0.6362 - val_loss: 1.2139 - val_accuracy: 0.5808\n",
            "Epoch 13/50\n",
            "782/782 [==============================] - 79s 101ms/step - loss: 1.0217 - accuracy: 0.6362 - val_loss: 1.1514 - val_accuracy: 0.5979\n",
            "Epoch 14/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 1.0078 - accuracy: 0.6428 - val_loss: 1.2255 - val_accuracy: 0.5851\n",
            "Epoch 15/50\n",
            "782/782 [==============================] - 76s 98ms/step - loss: 1.0065 - accuracy: 0.6454 - val_loss: 1.1429 - val_accuracy: 0.6010\n",
            "Epoch 16/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.9919 - accuracy: 0.6497 - val_loss: 1.1537 - val_accuracy: 0.5962\n",
            "Epoch 17/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.9871 - accuracy: 0.6521 - val_loss: 1.1438 - val_accuracy: 0.6016\n",
            "Epoch 18/50\n",
            "782/782 [==============================] - 77s 98ms/step - loss: 0.9814 - accuracy: 0.6524 - val_loss: 1.1554 - val_accuracy: 0.5972\n",
            "Epoch 19/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.9710 - accuracy: 0.6567 - val_loss: 1.1922 - val_accuracy: 0.5956\n",
            "Epoch 20/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.9660 - accuracy: 0.6598 - val_loss: 1.1468 - val_accuracy: 0.6046\n",
            "Epoch 21/50\n",
            "782/782 [==============================] - 78s 99ms/step - loss: 0.9542 - accuracy: 0.6637 - val_loss: 1.1374 - val_accuracy: 0.6056\n",
            "Epoch 22/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.9472 - accuracy: 0.6654 - val_loss: 1.1889 - val_accuracy: 0.5937\n",
            "Epoch 23/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 0.9513 - accuracy: 0.6634 - val_loss: 1.2074 - val_accuracy: 0.5879\n",
            "Epoch 24/50\n",
            "782/782 [==============================] - 75s 97ms/step - loss: 0.9369 - accuracy: 0.6688 - val_loss: 1.1684 - val_accuracy: 0.6058\n",
            "Epoch 25/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.9338 - accuracy: 0.6710 - val_loss: 1.1760 - val_accuracy: 0.6047\n",
            "Epoch 26/50\n",
            "782/782 [==============================] - 77s 98ms/step - loss: 0.9344 - accuracy: 0.6711 - val_loss: 1.1785 - val_accuracy: 0.6029\n",
            "Epoch 27/50\n",
            "782/782 [==============================] - 77s 98ms/step - loss: 0.9232 - accuracy: 0.6720 - val_loss: 1.2046 - val_accuracy: 0.6018\n",
            "Epoch 28/50\n",
            "782/782 [==============================] - 81s 103ms/step - loss: 0.9170 - accuracy: 0.6759 - val_loss: 1.1855 - val_accuracy: 0.5972\n",
            "Epoch 29/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 0.9122 - accuracy: 0.6774 - val_loss: 1.2093 - val_accuracy: 0.5945\n",
            "Epoch 30/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.9134 - accuracy: 0.6769 - val_loss: 1.1628 - val_accuracy: 0.6092\n",
            "Epoch 31/50\n",
            "782/782 [==============================] - 78s 99ms/step - loss: 0.9206 - accuracy: 0.6727 - val_loss: 1.1982 - val_accuracy: 0.5957\n",
            "Epoch 32/50\n",
            "782/782 [==============================] - 76s 98ms/step - loss: 0.8941 - accuracy: 0.6829 - val_loss: 1.1873 - val_accuracy: 0.6057\n",
            "Epoch 33/50\n",
            "782/782 [==============================] - 81s 103ms/step - loss: 0.9028 - accuracy: 0.6788 - val_loss: 1.2430 - val_accuracy: 0.5930\n",
            "Epoch 34/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.8982 - accuracy: 0.6820 - val_loss: 1.2499 - val_accuracy: 0.5888\n",
            "Epoch 35/50\n",
            "782/782 [==============================] - 78s 99ms/step - loss: 0.8972 - accuracy: 0.6817 - val_loss: 1.2250 - val_accuracy: 0.6064\n",
            "Epoch 36/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.8899 - accuracy: 0.6826 - val_loss: 1.2351 - val_accuracy: 0.5983\n",
            "Epoch 37/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 0.8837 - accuracy: 0.6872 - val_loss: 1.2352 - val_accuracy: 0.5963\n",
            "Epoch 38/50\n",
            "782/782 [==============================] - 81s 104ms/step - loss: 0.8776 - accuracy: 0.6916 - val_loss: 1.2313 - val_accuracy: 0.6000\n",
            "Epoch 39/50\n",
            "782/782 [==============================] - 78s 99ms/step - loss: 0.8792 - accuracy: 0.6880 - val_loss: 1.2833 - val_accuracy: 0.5788\n",
            "Epoch 40/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.8703 - accuracy: 0.6914 - val_loss: 1.2101 - val_accuracy: 0.6068\n",
            "Epoch 41/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.8796 - accuracy: 0.6901 - val_loss: 1.2914 - val_accuracy: 0.5756\n",
            "Epoch 42/50\n",
            "782/782 [==============================] - 78s 99ms/step - loss: 0.8622 - accuracy: 0.6950 - val_loss: 1.2469 - val_accuracy: 0.6043\n",
            "Epoch 43/50\n",
            "782/782 [==============================] - 79s 101ms/step - loss: 0.8514 - accuracy: 0.6992 - val_loss: 1.2624 - val_accuracy: 0.5965\n",
            "Epoch 44/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.8510 - accuracy: 0.6974 - val_loss: 1.3083 - val_accuracy: 0.5827\n",
            "Epoch 45/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 0.8581 - accuracy: 0.6963 - val_loss: 1.2393 - val_accuracy: 0.6106\n",
            "Epoch 46/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 0.8510 - accuracy: 0.6972 - val_loss: 1.3428 - val_accuracy: 0.5853\n",
            "Epoch 47/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.8430 - accuracy: 0.7044 - val_loss: 1.3282 - val_accuracy: 0.5836\n",
            "Epoch 48/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 0.8560 - accuracy: 0.6972 - val_loss: 1.3254 - val_accuracy: 0.5963\n",
            "Epoch 49/50\n",
            "782/782 [==============================] - 77s 99ms/step - loss: 0.8404 - accuracy: 0.7046 - val_loss: 1.2201 - val_accuracy: 0.6031\n",
            "Epoch 50/50\n",
            "782/782 [==============================] - 76s 97ms/step - loss: 0.8391 - accuracy: 0.7049 - val_loss: 1.3090 - val_accuracy: 0.5776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.Evaluate the model: Finally, you can evaluate the performance of the model on the test data using the evaluate method"
      ],
      "metadata": {
        "id": "VvJPp2WYt-ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHaJNCU8uBMi",
        "outputId": "ac9cab64-d111-4bd7-d57e-565355c71cc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 14ms/step - loss: 1.3090 - accuracy: 0.5776\n",
            "Test accuracy: 0.5776000022888184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion: In this project, we have used the CIFAR-10 dataset to build a convolutional neural network (CNN) for image classification. We have trained the model on a training set of 50,000 images and evaluated its performance on a test set of 10,000 images.\n",
        "\n",
        "Our CNN architecture consisted of three convolutional layers with max-pooling, followed by two fully connected layers. We used the softmax activation function in the output layer to produce probability distributions over the 10 possible classes in the CIFAR-10 dataset.\n",
        "\n",
        "We trained the model using the Adam optimizer and the categorical cross-entropy loss function. During training, we achieved an accuracy of around 60%  after 50 epochs, and thereafter an accuracy of 57% on the evaluation of test data, which indicates that the model is able to recognize and classify images in the CIFAR-10 dataset with reasonable accuracy.\n",
        "\n",
        "Overall, this project demonstrates the effectiveness of using convolutional neural networks for image classification tasks and highlights the importance of selecting appropriate hyperparameters and optimization techniques to achieve good performance on the test set."
      ],
      "metadata": {
        "id": "fC7gsJSPvYbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alternatively in order to improve the model's performance, one can:\n",
        "\n",
        "Increase the number of epochs during training. This allows the model to train for a longer period of time and potentially improve its accuracy. However, be careful not to overfit the model to the training data.\n",
        "\n",
        "Add regularization techniques, such as dropout or weight decay, to prevent overfitting.\n",
        "\n",
        "Increase the number of convolutional layers or filters in each layer to capture more complex features in the images.\n",
        "\n",
        "Adjust the learning rate of the optimizer to fine-tune the model's convergence.\n",
        "\n",
        "Try different optimization algorithms, such as RMSprop or SGD with momentum, to see if they improve the model's performance.\n",
        "\n",
        "Increase the batch size during training to allow the model to learn from more images per iteration.\n",
        "\n",
        "Augment the training data with techniques such as rotation, flipping, or zooming to increase the diversity of the data and prevent overfitting.\n",
        "\n",
        "Experimenting with these techniques can help you improve the accuracy of your model. Remember to also monitor the model's performance on the validation set during training to avoid overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xfGnDxcI54uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8KRYrYU06Mrc"
      }
    }
  ]
}